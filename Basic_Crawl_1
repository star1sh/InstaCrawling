from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import re
import pandas as pd

# step2.아이디, 비밀번호 설정
id = ""
pw = ""

keyword = "피프틴"
url = "https://www.instagram.com/explore/tags/"+ keyword 

# step3.크롬 웹드라이버 실행
driver = webdriver.Chrome(r"C:\Users\hagae\Desktop\instagram_Crawler\chromedriver.exe")
FIRST_IMG_CSS = "div.v1Nh3.KIKUG._bz0w"
# step4.인스타그램 로그인 함수 정의
def login(id, pw):
    # 로그인 페이지로 이동
    driver.get("https://www.instagram.com/accounts/login/")
    time.sleep(1)
    
    # id와 pw를 입력하는 창의 요소 정보 획득
    input = driver.find_elements(By.TAG_NAME,"input")

    # 아이디를 입력
    input[0].send_keys(id)

    # 비밀번호 입력
    input[1].send_keys(pw)

    # 엔터
    input[1].send_keys(Keys.RETURN)
    time.sleep(5)

    # 로그인 정보 저장 여부 팝업창 제거 ("나중에 하기 버튼 클릭")
    btn_later1 = driver.find_element(By.CLASS_NAME,'_ac8f')
    btn_later1.click()
    time.sleep(5)

    # 알림 설정 팝업창 제거 ("나중에 하기 버튼 클릭")
    btn_later2 = driver.find_element(By.CLASS_NAME,'_a9--._a9_1')
    btn_later2.click()

#첫번째 이미지 클릭
def click_first_image(driver):
    try:
        driver.find_elements(By.CLASS_NAME, '_aabd _aa8k  _al3l').click()
        is_first_img_click_success = True
    except:
        is_first_img_click_success = False

    return is_first_img_click_success



# step5.로그인 함수 실행
login(id,pw)

time.sleep(10)

driver.get(url)

time.sleep(10)

first_img_css = 'div._ac7v._aang > div._aabd._aa8k._aanf'

time.sleep(10)

#첫번째 게시물 클릭
post = driver.find_elements(By.CSS_SELECTOR,"div._aagw")
post[0].click()
time.sleep(10)

#다음피드로 이동
def move_next(driver):
    right = driver.find_element(By.CSS_SELECTOR,"div._aaqg._aaqh")
    right.click()
    time.sleep(3)

from bs4 import BeautifulSoup

#본문내용,작성일자,좋아요수,위치정보,해시태그 가져오기
def get_content(driver):
    html = driver.page_source
    #빠른탐색을 위해 html대신 lxml 사용
    soup = BeautifulSoup(html, 'lxml')
    try:
        content = soup.select('div._a9zs')[0].text
    except:
        content=''
    
    tags = re.findall(r'#[^\s#,\\]+', content)

    date = soup.select('time._aaqe')[0]['datetime'][:10]

    try:
        like = soup.select('div._aacl._aaco._aacx._aada._aade')[0].findAll('span')[-1].text
    except:
        like = 0
    
    try:
        place = soup.select('div._aaqm')[0].text

    except:
        place = ''
    data = [content, date, like, place,tags]
    return data

#데이터 수집 시작
results = []
target = 10
for i in range(target):
    try:
        data = get_content(driver)
        results.append(data)
        move_next(driver)
    except:
        time.sleep(2)
        move_next(driver)
    time.sleep(5)
# data = get_content(driver)
# results.append(data)

print(results[:2])

#엑셀파일로 저장
import pandas as pd
from datetime import datetime

date = datetime.today().strftime('%Y-%m-%d')

results_df = pd.DataFrame(results)
results_df.colums = ['content','date','like','place','tags']
results_df.to_excel(date + '_about' + keyword+' insta crawling.xlsx')
